{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1I9Lu03ZLKK"
   },
   "source": [
    "## Introduction\n",
    "Here we train our tumor segmentation network. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y26OyK-MZa0l"
   },
   "source": [
    "## Imports:\n",
    "\n",
    "* Pathlib for easy path handling\n",
    "* torch for tensor handling\n",
    "* pytorch lightning for efficient and easy training implementation\n",
    "* ModelCheckpoint and TensorboardLogger for checkpoint saving and logging\n",
    "* imgaug for Data Augmentation\n",
    "* numpy for file loading and array ops\n",
    "* matplotlib for visualizing some images\n",
    "* tqdm for progress par when validating the model\n",
    "* celluloid for easy video generation\n",
    "* Our dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5Jg6BIiY7B0"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfyXT1dGb4Nl",
    "outputId": "0555bc01-f7a8-4003-f34f-a5168671b0d1"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib.util, os\n",
    "\n",
    "# put the folder with dataset.py/model.py on sys.path\n",
    "import sys\n",
    "sys.path.insert(0, r\"E:/DoNotTouch/projects/LANSCLC/CIS_5810\")\n",
    "\n",
    "import dataset   # loads dataset.py\n",
    "import model     # loads model.py\n",
    "\n",
    "LungDataset = dataset.LungDataset\n",
    "UNet        = model.UNet\n",
    "\n",
    "print(LungDataset.__module__)  # should print \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL_1a6y5Z1hF"
   },
   "source": [
    "## Dataset Creation\n",
    "Here we create the train and validation dataset. <br />\n",
    "Additionally we define our data augmentation pipeline.\n",
    "Subsequently the two dataloaders are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbuKPhcEZywS"
   },
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(translate_percent=(0.15),\n",
    "               scale=(0.85, 1.15), # zoom in or out\n",
    "               rotate=(-45, 45)#\n",
    "               ),  # rotate up to 45 degrees\n",
    "    iaa.ElasticTransformation()  # Elastic Transformations\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7G3eGAcaRMB",
    "outputId": "e094e872-3d07-427f-a5f6-c5893c01b458"
   },
   "outputs": [],
   "source": [
    "# Create the dataset objects\n",
    "BASE = Path(\"E:/DoNotTouch/projects/LANSCLC/CIS_5810/selected_150_split\")\n",
    "train_path = BASE / \"Preprocessed_for_2D_Unet/train\"\n",
    "val_path = BASE / \"Preprocessed_for_2D_Unet/val\"\n",
    "test_path = BASE / \"Preprocessed_for_2D_Unet/test\"\n",
    "\n",
    "train_dataset = LungDataset(train_path, seq)\n",
    "val_dataset = LungDataset(val_path, None)\n",
    "test_dataset = LungDataset(test_path, None)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} train images, {len(val_dataset)} val images and {len(test_dataset)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZyjhmowniFt"
   },
   "source": [
    "## Oversampling to tackle strong class imbalance\n",
    "Lung tumors are often very small, thus we need to make sure that our model does not learn a trivial solution which simply outputs 0 for all voxels.<br />\n",
    "In this notebook we use oversampling to sample slices which contain a tumor more often.\n",
    "\n",
    "To do so we can use the WeightedRandomSampler provided by pytorch which needs a weight for each sample in the dataset.\n",
    "Typically we have one weight for each class, which means that we need to calculate two weights, one for slices without tumors and one for slices with a tumor and create list that assigns each sample from the dataset the corresponding weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS_UVcUJn2EX"
   },
   "source": [
    "To do so, we at first need to create a list containing only the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "f76444086544492c8d4b5e27c97c4bdb",
      "ad5e0691e3b94019891483dfbf5d44fe",
      "0962cfe5526e4ee78e0a11fd7c156815",
      "5b9f0046f8b7493cbb92734ef60fec21",
      "f85fe98565f241b496c3ac0ceaa70bb8",
      "4065b2487daf4eafa756a263d448bf07",
      "9a524a66999b4e30bfab0ec99b07c212",
      "07b4b9f765144de1a4017f713506250b",
      "edac09c117b14d6c80516875bfc063ed",
      "255aabac5f404310a9c8bd40ac9f52e2",
      "0ce199b67fad480184ff2a68e7e4576f"
     ]
    },
    "id": "Y7xzvMQCnKYk",
    "outputId": "4e7e995e-a987-46ae-8ee5-9cae31073098"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def disable_aug(ds):\n",
    "    prev = getattr(ds, \"augment_params\", None)\n",
    "    try:\n",
    "        ds.augment_params = None\n",
    "        yield ds\n",
    "    finally:\n",
    "        ds.augment_params = prev\n",
    "\n",
    "def has_tumor(mask, thr=0.5):\n",
    "    a = np.asarray(mask)\n",
    "    a = np.squeeze(a)\n",
    "    if a.ndim == 3:               # collapse channel dim if present\n",
    "        a = (a > thr).any(axis=0)\n",
    "    return int((a > thr).any())\n",
    "\n",
    "with disable_aug(train_dataset):\n",
    "    target_list = []\n",
    "    for i in tqdm(range(len(train_dataset))):\n",
    "        _, label = train_dataset[i]     # now no imgaug/warpAffine is triggered\n",
    "        target_list.append(has_tumor(label))\n",
    "\n",
    "pos = sum(target_list)\n",
    "print(f\"positives: {pos}/{len(target_list)} ({100*pos/len(target_list):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yI-lYSUKyxV"
   },
   "source": [
    "Then we can calculate the weight for each class: To do so, we can simply compute the fraction between the classes and then create the weight list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snnSp65pztFZ"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y opencv-python opencv-contrib-python opencv-python-headless thinc\n",
    "# !pip install -U --force-reinstall --no-cache-dir numpy==1.26.4 scipy==1.11.4\n",
    "# !pip install -U imgaug==0.4.0\n",
    "# !pip install -U \"opencv-python==4.8.1.78\" \"opencv-contrib-python==4.8.1.78\" \"opencv-python-headless==4.8.1.78\"\n",
    "# # (Only if you truly need thinc/spaCy here)\n",
    "# # pip install \"thinc<8.3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AT2Hoyoc3-yP",
    "outputId": "d0d311e7-94e8-4100-aa03-b2ac98e3646f"
   },
   "outputs": [],
   "source": [
    "uniques = np.unique(target_list, return_counts=True)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXX_aT7iLLNl",
    "outputId": "7afdda7a-fd3f-437e-ec47-d244767c6f9e"
   },
   "outputs": [],
   "source": [
    "fraction = uniques[1][0] / uniques[1][1]\n",
    "fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2F3QFfgLO4Z"
   },
   "source": [
    "Subsequently we assign the weight 1 to each slice without a tumor and ~9 to each slice with a tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vB0nxaQ3LNg_"
   },
   "outputs": [],
   "source": [
    "weight_list = []\n",
    "for target in target_list:\n",
    "    if target == 0:\n",
    "        weight_list.append(1)\n",
    "    else:\n",
    "        weight_list.append(fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7rLsqwSLd4i",
    "outputId": "d53cdebf-09b8-4878-8ee6-e26e7ef7c116"
   },
   "outputs": [],
   "source": [
    "weight_list[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kREEF79hLkNQ"
   },
   "source": [
    "Finally we create the sampler which we can pass to the DataLoader. We only use a sampler for the train loader. We dont't want to change the validation data to get a real validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32HbCQe-Lfu7"
   },
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weight_list, len(weight_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIXJ2Fvrd0r2",
    "outputId": "ce983a72-3ad2-44e1-cf86-4197a1711f1a"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 23\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, sampler=sampler, pin_memory=True, persistent_workers=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3YwzlJgeBK3"
   },
   "source": [
    "We can verify that our sampler works by taking a batch from the train loader and count how many labels are larger than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaCjjvPdd23e"
   },
   "outputs": [],
   "source": [
    "verify_sampler = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMWeZ4FEeSEW",
    "outputId": "2122c5dd-0c4f-4b86-9345-36211e1439e3"
   },
   "outputs": [],
   "source": [
    "(verify_sampler[1][:,0]).sum([1, 2]) > 0  # ~ half the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB8FPSbXeZld"
   },
   "source": [
    "## Loss\n",
    "\n",
    "We use the Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hb6b-vfCevlZ"
   },
   "source": [
    "## Full Segmentation Model\n",
    "\n",
    "We now combine everything into the full pytorch lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpyp_d5oeUaP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TumorSegmentation(pl.LightningModule):\n",
    "    def __init__(self, lr: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.model = UNet().float()                 # ensure model params are float32\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()       # expects float targets, logits input\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)                        # logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ct, mask = batch\n",
    "        ct   = ct.float()                           # <— enforce float32\n",
    "        mask = mask.float()                         # <— enforce float32\n",
    "\n",
    "        logits = self(ct)                           # (B,1,H,W) logits (float32)\n",
    "        loss = self.loss_fn(logits, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        if (batch_idx % 50 == 0) and self.logger is not None:\n",
    "            self.log_images(ct, logits, mask, \"Train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ct, mask = batch\n",
    "        ct   = ct.float()                           # <— enforce float32\n",
    "        mask = mask.float()                         # <— enforce float32\n",
    "\n",
    "        logits = self(ct)\n",
    "        val_loss = self.loss_fn(logits, mask)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        if (batch_idx % 50 == 0) and self.logger is not None:\n",
    "            self.log_images(ct, logits, mask, \"Val\")\n",
    "        return val_loss\n",
    "\n",
    "    def log_images(self, ct, logits, mask, split_name: str):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pred_bin = (probs > 0.5)\n",
    "\n",
    "        img = ct[0, 0].detach().cpu().numpy()\n",
    "        gt  = mask[0, 0].detach().cpu().numpy()\n",
    "        pr  = pred_bin[0, 0].detach().cpu().numpy()\n",
    "\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axis[0].imshow(img, cmap=\"bone\")\n",
    "        axis[0].imshow(np.ma.masked_where(gt == 0, gt), alpha=0.6)\n",
    "        axis[0].set_title(\"Ground Truth\"); axis[0].axis(\"off\")\n",
    "\n",
    "        axis[1].imshow(img, cmap=\"bone\")\n",
    "        axis[1].imshow(np.ma.masked_where(pr == 0, pr), alpha=0.6, cmap=\"autumn\")\n",
    "        axis[1].set_title(\"Prediction\"); axis[1].axis(\"off\")\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"{split_name} Prediction vs Label\", fig, self.global_step)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sf6XtOfue63w"
   },
   "outputs": [],
   "source": [
    "# Instanciate the model\n",
    "model = TumorSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coJUDYYve9O3"
   },
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Dice',\n",
    "    save_top_k=100,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVSBFPkIe_N2",
    "outputId": "66fc9f28-2c09-4b35-a9d0-d141736fd33e"
   },
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./ckpts\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", devices=1,            # or accelerator=\"auto\", devices=\"auto\"\n",
    "    logger=TensorBoardLogger(save_dir=\"./logs\"),\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743,
     "referenced_widgets": [
      "4ac78f2d22534ec5965d4afedd96fd58",
      "90f51cc8de6747549997fd7c0eda4d8e",
      "420d237a71414f529be993daf8eb8dee",
      "e67a5188ca364c3a9d1487e86ea0a162",
      "71fd99cbc2ae4314837ba20aa628b8e5",
      "471e6f8b6b87484490dfb333717d8e63",
      "f8e571695a44499292aa0108fffc56cb",
      "b3c6d9d6550c4f5ab7cd675ce6248473",
      "ec2789b8dcf54e268577fe22352a420e",
      "93288b35757f4bcd8f8c50bb5cb95b54",
      "5a7da7d3672d46d687b3cecfa85c93d8",
      "65fc8d5dbcdc45e6bc169cacbb044ca2",
      "015a82fe65974eda9f34cab8774241b0",
      "150189782f64401bb3a92366e2f562d9",
      "d7cf3b26108b406eb9e7a11c918291f8",
      "620dfcf8289a46bf9ebadd98e01f40c3",
      "53653f3b9186464ba6bc687c232d6f15",
      "a3d5b6297e3b4755b95169e177e3829b",
      "acfdf92655554e5fa3e175f597a4a82c",
      "ef6368ba488b46b480ddb2fd25992313",
      "efc0fb57d6d749bb928b5c22be3dc78a",
      "8f857ad0f68d4e5d9a8c53b51ad4eb95",
      "9a201b497aad43b6a27a62eba3593736",
      "7dce967c4cfc41b687b53ac24cbafc0e",
      "6510f2c063e54f3bba2136b87e68d093",
      "f61471b85c1c4e42bfb21fbc6319abe1",
      "cb5853c3dcda4107b08c0e398babd381",
      "8bd2724541764038befb9ac0052d2024",
      "9222c87b40cc4d4cbc625d778a907e5f",
      "a09d4823cf364855a37a7906aa324237",
      "d88c2167346b4b85b4922c06a6c0008e",
      "a85e1086700246918cda3f5f189d49f2",
      "7a579a50d9b545c6ab67aaa4d1091ad0"
     ]
    },
    "id": "WPQ9_Qj8h15F",
    "outputId": "4f25fb0b-45ff-4910-92e4-0fbe457e6f35"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJhuEBYUa9c_"
   },
   "source": [
    "## Evaluation:\n",
    "Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qitOoP8EYqSR"
   },
   "outputs": [],
   "source": [
    "class DiceScore(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    class to compute the Dice Loss\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        pred = torch.flatten(pred)\n",
    "        mask = torch.flatten(mask)\n",
    "\n",
    "        counter = (pred * mask).sum()  # Counter\n",
    "        denum = pred.sum() + mask.sum()  # denominator\n",
    "        dice = (2*counter)/denum\n",
    "\n",
    "        return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCrSXmq5bC_y"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def resolve_ckpt_path(ckpt_cb) -> Path:\n",
    "    # 1) Prefer the \"best\" checkpoint\n",
    "    p = getattr(ckpt_cb, \"best_model_path\", \"\") or \"\"\n",
    "    if p and Path(p).is_file():\n",
    "        return Path(p)\n",
    "\n",
    "    # 2) Fall back to \"last\"\n",
    "    p = getattr(ckpt_cb, \"last_model_path\", \"\") or \"\"\n",
    "    if p and Path(p).is_file():\n",
    "        return Path(p)\n",
    "\n",
    "    # 3) Search the callback dirpath for any .ckpt (pick most recent)\n",
    "    root = Path(getattr(ckpt_cb, \"dirpath\", \".\"))  # where ModelCheckpoint saves\n",
    "    cands = list(root.rglob(\"*.ckpt\")) if root.exists() else []\n",
    "    if not cands:\n",
    "        # also search Lightning default logs/checkpoints tree\n",
    "        cands = list(Path(\"lightning_logs\").rglob(\"*.ckpt\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\n",
    "            \"No checkpoint found. Ensure ModelCheckpoint has dirpath/monitor or use save_last=True.\"\n",
    "        )\n",
    "    return max(cands, key=lambda x: x.stat().st_mtime)\n",
    "\n",
    "# --- use it ---\n",
    "ckpt_path = resolve_ckpt_path(checkpoint_callback)\n",
    "print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "model = TumorSegmentation.load_from_checkpoint(\n",
    "    ckpt_path,\n",
    "    map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # strict=True  # set to False if your model signature changed since saving\n",
    ")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBVXoEMddszz"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for slice, label in tqdm(val_dataset):\n",
    "    slice = torch.tensor(slice).float().to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        pred = torch.sigmoid(model(slice))\n",
    "    preds.append(pred.cpu().numpy())\n",
    "    labels.append(label)\n",
    "\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6FT407Feof9"
   },
   "source": [
    "Compute overall Dice Score on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJI470ZCd98X"
   },
   "outputs": [],
   "source": [
    "dice_score = DiceScore()(torch.from_numpy(preds), torch.from_numpy(labels).unsqueeze(0).float())\n",
    "print(f\"The Val Dice Score is: {dice_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fYFLAaH0TDY"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for slice, label in tqdm(test_dataset):\n",
    "    slice = torch.tensor(slice).float().to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        pred = torch.sigmoid(model(slice))\n",
    "    preds.append(pred.cpu().numpy())\n",
    "    labels.append(label)\n",
    "\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpZhSLOBRsKN"
   },
   "source": [
    "Compute overall Dice Score on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKBbD3HXRmUS"
   },
   "outputs": [],
   "source": [
    "dice_score = DiceScore()(torch.from_numpy(preds), torch.from_numpy(labels).unsqueeze(0).float())\n",
    "print(f\"The Test Dice Score is: {dice_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _dice_coef(pred_bin: np.ndarray, gt_bin: np.ndarray, eps=1e-7) -> float:\n",
    "    inter = np.logical_and(pred_bin, gt_bin).sum()\n",
    "    denom = pred_bin.sum() + gt_bin.sum()\n",
    "    if denom == 0:\n",
    "        return 1.0\n",
    "    return (2.0 * inter + eps) / (denom + eps)\n",
    "\n",
    "def _get_paths_attr(ds):\n",
    "    for name in (\"all_files\", \"files\", \"paths\", \"slices\"):\n",
    "        if hasattr(ds, name):\n",
    "            return getattr(ds, name)\n",
    "    raise AttributeError(\"Dataset must expose a list of slice paths via \"\n",
    "                         \"`all_files`, `files`, `paths`, or `slices`.\")\n",
    "\n",
    "def _pid_from_slice_path(p: Path) -> str:\n",
    "    # .../<patient>/data/<slice>.npy\n",
    "    return p.parent.parent.name\n",
    "\n",
    "def _slice_idx_from_path(p: Path) -> int:\n",
    "    return int(p.stem)\n",
    "\n",
    "def _ensure_img_tensor(x) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns torch.float32 tensor of shape (1,H,W) on CPU.\n",
    "    Accepts numpy or torch; shapes (H,W) or (1,H,W) or (C,H,W) with C==1.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(x):\n",
    "        arr = x.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(x)\n",
    "\n",
    "    # squeeze extra leading dims but keep spatial\n",
    "    while arr.ndim > 2 and arr.shape[0] == 1:\n",
    "        arr = arr[1-1] if arr.ndim == 3 else arr.squeeze(axis=0)\n",
    "    if arr.ndim == 2:\n",
    "        arr = arr[None, ...]  # (1,H,W)\n",
    "    elif arr.ndim == 3 and arr.shape[0] != 1:\n",
    "        raise ValueError(f\"Expected 1 channel, got shape {arr.shape}\")\n",
    "\n",
    "    return torch.from_numpy(arr.astype(np.float32, copy=False))  # (1,H,W)\n",
    "\n",
    "def _ensure_mask_np(y) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns float32 numpy array of shape (H,W).\n",
    "    Accepts numpy or torch; shapes (H,W) or (1,H,W).\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(y):\n",
    "        arr = y.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(y)\n",
    "    # squeeze a single channel if present\n",
    "    if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "        arr = arr[0]\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"Mask must be 2D or (1,H,W); got {arr.shape}\")\n",
    "    return arr.astype(np.float32, copy=False)\n",
    "\n",
    "# ---------- main ----------\n",
    "def eval_by_patient(dataset, model, threshold=0.5, device=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Groups slices per patient using dataset slice paths and computes volumetric Dice.\n",
    "    Works whether dataset returns numpy arrays or torch tensors.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    paths = _get_paths_attr(dataset)\n",
    "    by_patient = defaultdict(list)  # pid -> list of (slice_idx, prob2d, gt2d)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(dataset)):\n",
    "            x, y = dataset[idx]                     # may be numpy or torch\n",
    "            img_t = _ensure_img_tensor(x)           # (1,H,W) float32\n",
    "            gt2d  = _ensure_mask_np(y)              # (H,W) float32\n",
    "\n",
    "            p = Path(str(paths[idx]))\n",
    "            pid = _pid_from_slice_path(p)\n",
    "            sidx = _slice_idx_from_path(p)\n",
    "\n",
    "            # model expects (N,C,H,W)\n",
    "            logits = model(img_t.unsqueeze(0).to(device))  # (1,C,H,W)\n",
    "            if logits.shape[1] == 1:\n",
    "                prob2d = torch.sigmoid(logits)[0, 0].cpu().numpy()\n",
    "            else:\n",
    "                prob2d = torch.softmax(logits, dim=1)[0, 1].cpu().numpy()\n",
    "\n",
    "            by_patient[pid].append((sidx, prob2d.astype(np.float32, copy=False), gt2d))\n",
    "\n",
    "    patient_dice = {}\n",
    "    patient_pred_vols, patient_gt_vols = {}, {}\n",
    "\n",
    "    for pid, triples in by_patient.items():\n",
    "        triples.sort(key=lambda t: t[0])\n",
    "        pred_vol = np.stack([t[1] for t in triples], axis=0)  # (D,H,W)\n",
    "        gt_vol   = np.stack([t[2] for t in triples], axis=0)\n",
    "\n",
    "        pred_bin = pred_vol >= float(threshold)\n",
    "        gt_bin   = gt_vol  >  0.0\n",
    "\n",
    "        patient_pred_vols[pid] = pred_vol\n",
    "        patient_gt_vols[pid]   = gt_vol\n",
    "        patient_dice[pid]      = _dice_coef(pred_bin, gt_bin)\n",
    "\n",
    "    if verbose and patient_dice:\n",
    "        vals = np.array(list(patient_dice.values()), dtype=np.float32)\n",
    "        print(f\"Patients: {len(vals)} | mean Dice: {vals.mean():.4f} | median: {np.median(vals):.4f}\")\n",
    "    return patient_dice, patient_pred_vols, patient_gt_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dice, val_pred_vols, val_gt_vols = eval_by_patient(val_dataset, model, threshold=0.5)\n",
    "val_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dice, test_pred_vols, test_gt_vols = eval_by_patient(test_dataset, model, threshold=0.5)\n",
    "test_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UR0RC8oe3Qb"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcHcDjw1evkG"
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6L18tmHfL3Y"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYVxP_1WfNSY"
   },
   "outputs": [],
   "source": [
    "BASE = Path(\"E:/DoNotTouch/projects/LANSCLC/CIS_5810/selected_150_split\")\n",
    "\n",
    "subject = Path(BASE / \"test\" / \"image\" / \"Lung_045_0000.nii.gz\")\n",
    "ct = nib.load(subject).get_fdata() / 3071  # standardize\n",
    "ct = ct[:,:,30:]  # crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ooXwhzMhK5f"
   },
   "outputs": [],
   "source": [
    "segmentation = []\n",
    "label = []\n",
    "scan = []\n",
    "\n",
    "for i in range(ct.shape[-1]):\n",
    "    slice = ct[:,:,i]\n",
    "    slice = cv2.resize(slice, (256, 256))\n",
    "    slice = torch.tensor(slice)\n",
    "    scan.append(slice)\n",
    "    slice = slice.unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(slice)[0][0].cpu()\n",
    "    pred = pred > THRESHOLD\n",
    "    segmentation.append(pred)\n",
    "    label.append(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxtSbce3hTew"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "\n",
    "def reorient_2d(a):\n",
    "    # flip vertically, then rotate -90° (clockwise)\n",
    "    return np.rot90(np.flipud(a), k=3)\n",
    "\n",
    "def to_slice_list(x):\n",
    "    \"\"\"\n",
    "    Return a list of 2D numpy arrays (same shape).\n",
    "    Handles: torch.Tensor, list/tuple of slices, or a 3D volume (D,H,W) / (H,W,D).\n",
    "    \"\"\"\n",
    "    # torch -> numpy\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "\n",
    "    # squeeze singleton axes (e.g., [1, D, H, W] -> [D, H, W])\n",
    "    arr = np.array(x, dtype=object)  # allow ragged temporarily\n",
    "    try:\n",
    "        arr = np.squeeze(np.asarray(x))\n",
    "    except Exception:\n",
    "        # if x is a ragged list, skip to list-branch below\n",
    "        arr = None\n",
    "\n",
    "    slices = None\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        # list/tuple of slices\n",
    "        slices = [np.asarray(s) for s in x]\n",
    "    elif isinstance(arr, np.ndarray):\n",
    "        if arr.ndim == 2:\n",
    "            slices = [arr]\n",
    "        elif arr.ndim == 3:\n",
    "            # decide which axis is depth (use the one with the smallest size heuristically)\n",
    "            # then move it to front so we have (D, H, W)\n",
    "            d_axis = int(np.argmin(arr.shape))\n",
    "            vol = np.moveaxis(arr, d_axis, 0)  # (D,H,W)\n",
    "            slices = [vol[i] for i in range(vol.shape[0])]\n",
    "        else:\n",
    "            raise ValueError(f\"Expected 2D/3D input or list of 2D slices; got array with ndim={arr.ndim}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type for scan/segmentation.\")\n",
    "\n",
    "    # verify uniform shapes\n",
    "    H, W = np.asarray(slices[0]).shape\n",
    "    for i, s in enumerate(slices):\n",
    "        if np.asarray(s).ndim != 2 or np.asarray(s).shape != (H, W):\n",
    "            raise ValueError(f\"Slice {i} has shape {np.asarray(s).shape}, \"\n",
    "                             f\"but expected {(H, W)}. Resize/crop first so all slices match.\")\n",
    "\n",
    "    # ensure base float arrays\n",
    "    return [np.asarray(s, dtype=np.float32, order=\"C\") for s in slices]\n",
    "\n",
    "# -------- build consistent slice lists --------\n",
    "scan_slices = to_slice_list(scan)\n",
    "seg_slices  = to_slice_list(segmentation)\n",
    "\n",
    "# optional: ensure seg is binary/integer for masking\n",
    "seg_slices  = [(s > 0).astype(np.uint8) for s in seg_slices]\n",
    "\n",
    "N = min(len(scan_slices), len(seg_slices))\n",
    "\n",
    "# reorient\n",
    "scan_r = [reorient_2d(s) for s in scan_slices[:N]]\n",
    "seg_r  = [reorient_2d(s) for s in seg_slices[:N]]\n",
    "\n",
    "# -------- animate --------\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "im = ax.imshow(scan_r[0], cmap=\"bone\", interpolation=\"nearest\")\n",
    "ov = ax.imshow(np.ma.masked_where(seg_r[0]==0, seg_r[0]),\n",
    "               cmap=\"autumn\", alpha=0.5, interpolation=\"nearest\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "def update(i):\n",
    "    im.set_data(scan_r[i])\n",
    "    ov.set_data(np.ma.masked_where(seg_r[i]==0, seg_r[i]))\n",
    "    return im, ov\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, frames=range(0, N, 2), interval=80, blit=False)\n",
    "display(HTML(anim.to_jshtml()))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWLGg8l0hWkA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
