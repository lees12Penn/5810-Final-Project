{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we trained the 3D Unet to segment primary lung tumors in CT scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* pathlib for easy path handling\n",
    "* HTML for visualizing volume videos\n",
    "* torchio for dataset creation\n",
    "* torch for DataLoaders, optimizer and loss\n",
    "* pytorch-lightning for training\n",
    "* numpy for masking\n",
    "* matplotlib for visualization\n",
    "* Our 3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torchio as tio\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "We can loop over all available scans and add them to the subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torchio as tio\n",
    "\n",
    "# --- Base split root ---\n",
    "BASE = Path(r\"E:\\DoNotTouch\\projects\\LANSCLC\\CIS_5810\\selected_150_split\")\n",
    "\n",
    "def change_img_to_label_path(p: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Map:\n",
    "      .../image/Lung_xxx_0000.nii.gz  -->  .../label_gtvp/Lung_xxx.nii.gz\n",
    "    \"\"\"\n",
    "    if p.parent.name != \"image\":\n",
    "        raise ValueError(f\"Expected parent folder named 'image', got: {p.parent}\")\n",
    "    label_dir = p.parent.with_name(\"label_gtvp\")\n",
    "    name = p.name\n",
    "    if name.endswith(\"_0000.nii.gz\"):\n",
    "        name = name[:-len(\"_0000.nii.gz\")] + \".nii.gz\"\n",
    "    else:\n",
    "        name = name.replace(\"_0000\", \"\")\n",
    "        if not name.endswith(\".nii.gz\"):\n",
    "            name += \".nii.gz\"\n",
    "    return label_dir / name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subjects(split_dir: Path, require_label: bool = True):\n",
    "    \"\"\"\n",
    "    Create TorchIO subjects from a split directory (train/val/test).\n",
    "    If require_label=False, subjects without labels are still created (CT only).\n",
    "    \"\"\"\n",
    "    image_paths = sorted(split_dir.rglob(r\"image/Lung_*_0000.nii.gz\"))\n",
    "    subjects, missing = [], 0\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        lab_path = change_img_to_label_path(img_path)\n",
    "        if lab_path.exists():\n",
    "            subjects.append(\n",
    "                tio.Subject(\n",
    "                    CT=tio.ScalarImage(img_path),\n",
    "                    Label=tio.LabelMap(lab_path),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            missing += 1\n",
    "            if require_label:\n",
    "                print(f\"[WARN] Missing label for: {img_path.name} -> {lab_path.name}\")\n",
    "                continue\n",
    "            # Test-time without labels:\n",
    "            subjects.append(tio.Subject(CT=tio.ScalarImage(img_path)))\n",
    "\n",
    "    print(f\"[{split_dir.name}] Built {len(subjects)} subjects (missing labels: {missing})\")\n",
    "    return subjects\n",
    "\n",
    "# --- Build each split ---\n",
    "train_subjects = build_subjects(BASE / \"train\", require_label=True)\n",
    "val_subjects   = build_subjects(BASE / \"val\",   require_label=True)\n",
    "test_subjects  = build_subjects(BASE / \"test\",  require_label=True)  # flip to True if you do have test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    assert subject[\"CT\"].orientation == (\"L\", \"P\", \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We use the same  augmentation steps as used in the Dataset notebook. <br />\n",
    "Regarding the processing, we use the **CropOrPad** functionality which crops or pads all images and masks to the same shape. <br />\n",
    "\n",
    "We use ($256 \\times 256 \\times 200$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = tio.Compose([\n",
    "            tio.CropOrPad((256, 256, 200)),\n",
    "            tio.RescaleIntensity((-1, 1))\n",
    "            ])\n",
    "\n",
    "\n",
    "augmentation = tio.RandomAffine(scales=(0.9, 1.1), degrees=(-10, 10))\n",
    "\n",
    "\n",
    "val_transform = process\n",
    "test_transform = process\n",
    "train_transform = tio.Compose([process, augmentation])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Define the train and validation dataset. We use 70 subjects for training and 40 for validation. <br />\n",
    "In order to help the segmentation network learn, we use the LabelSampler with p=0.2 for background, p=0.8 for lung tumors with a patch size of ($96 \\times 96 \\times 96$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tio.SubjectsDataset(train_subjects, transform=train_transform)\n",
    "val_dataset = tio.SubjectsDataset(val_subjects, transform=val_transform)\n",
    "test_dataset = tio.SubjectsDataset(test_subjects, transform=test_transform)\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size=96, label_name=\"Label\", label_probabilities={0:0.2, 1:0.8})\n",
    "#sampler = tio.data.UniformSampler(patch_size=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Create the queue to draw patches from.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches_queue = tio.Queue(\n",
    "     train_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=23,\n",
    "    )\n",
    "\n",
    "val_patches_queue = tio.Queue(\n",
    "     val_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=23,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Define train and val loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_amp = True  # set False if you don't use autocast\n",
    "\n",
    "train_bs = 4 if use_amp else 2\n",
    "val_bs   = min(8, 2*train_bs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_patches_queue, batch_size=train_bs, num_workers=0, pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_patches_queue,   batch_size=val_bs,   num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Finally we can create the Segmentation model.\n",
    "\n",
    "We use the Adam optimizer with a learning rate of 1e-4 and a weighted cross-entropy loss, which assigns a threefold increased loss to tumorous voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = UNet()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _compute_loss_and_pred(self, logits, mask):\n",
    "        \"\"\"\n",
    "        logits: [N,C,D,H,W]\n",
    "        mask:   from TorchIO -> [N,1,D,H,W] with {0,1}\n",
    "        \"\"\"\n",
    "        if logits.shape[1] == 1:\n",
    "            # binary case\n",
    "            target = mask.float()                     # [N,1,D,H,W]\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, target)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            pred = (probs > 0.5).float()             # [N,1,D,H,W]\n",
    "        else:\n",
    "            # multi-class case (C>1)\n",
    "            target = mask.squeeze(1).long()          # [N,D,H,W]\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            pred = torch.argmax(logits, dim=1, keepdim=True).float()  # [N,1,D,H,W]\n",
    "        return loss, pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img  = batch[\"CT\"][\"data\"].float()          # [N,1,D,H,W]\n",
    "        mask = batch[\"Label\"][\"data\"]               # [N,1,D,H,W]\n",
    "        logits = self(img)\n",
    "        loss, _ = self._compute_loss_and_pred(logits, mask)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img  = batch[\"CT\"][\"data\"].float()\n",
    "        mask = batch[\"Label\"][\"data\"]\n",
    "        logits = self(img)\n",
    "        loss, _ = self._compute_loss_and_pred(logits, mask)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model\n",
    "model = Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=100,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=\"./logs\")\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if use_gpu else \"cpu\",\n",
    "    devices=1 if use_gpu else None,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=1,\n",
    "    max_epochs=100,\n",
    "    precision=16 if use_gpu else 32,  # AMP on GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "# This might take some hours depending on your GPU\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "First we load the model and place it on the gpu if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_path  = checkpoint_callback.best_model_path\n",
    "best_score = checkpoint_callback.best_model_score.item() if checkpoint_callback.best_model_score is not None else None\n",
    "print(\"Best:\", best_path, best_score)\n",
    "\n",
    "model = Segmenter.load_from_checkpoint(best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Patch Aggregation\n",
    "The model was trained in a patch wise manner as the full volumes are too large to be placed on a typical GPU.\n",
    "But we still want to get a result for the whole volume.<br />\n",
    "We performed *Patch Aggregation* by using torchio.\n",
    "\n",
    "The goal of patch aggregation is to split the image into patches, then compute the segmentation for each patch and finally merge the predictions into the prediction for the full volume.\n",
    "\n",
    "The pipeline we performed is as follows:\n",
    "1. Define the **GridSampler(subject, patch_size, patch_overlap)** responsible for dividing the volume into patches. Each patch is defined by its location accesible via *tio.LOCATION*\n",
    "2. Define the **GridAggregator(grid_sampler)** which merges the predicted patches back together\n",
    "3. Compute the prediction on the patches and aggregate them via **aggregator.add_batch(pred, location)**\n",
    "4. Extract the full prediction via **aggregator.get_output_tensor()**\n",
    "\n",
    "Additionally, we leveraged the DataLoader from pytorch to perform the prediction in a batch wise manner for a nice speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a validation subject and extract the images and segmentation for evaluation\n",
    "IDX = 12\n",
    "mask = val_dataset[IDX][\"Label\"][\"data\"]\n",
    "imgs = val_dataset[IDX][\"CT\"][\"data\"]\n",
    "\n",
    "# GridSampler\n",
    "grid_sampler = tio.inference.GridSampler(val_dataset[IDX], 96, (8, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridAggregator\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for speed up\n",
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        input_tensor = patches_batch['CT'][\"data\"].to(device)  # Get batch of patches\n",
    "        locations = patches_batch[tio.LOCATION]  # Get locations of patches\n",
    "        pred = model(input_tensor)  # Compute prediction\n",
    "        aggregator.add_batch(pred, locations)  # Combine predictions to volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the volume prediction\n",
    "output_tensor = aggregator.get_output_tensor()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Finally we can visualize the prediction as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# -------- helpers --------\n",
    "def to_np(x):\n",
    "    return x.detach().cpu().numpy() if hasattr(x, \"detach\") else np.asarray(x)\n",
    "\n",
    "def make_depth_slicer(vol, D):\n",
    "    a = to_np(vol)\n",
    "    if a.ndim == 5:  # [B,C,H,W,D] or [B,C,D,H,W]\n",
    "        b = a[0]\n",
    "        if b.shape[-1] == D:   return lambda i: np.squeeze(b[0, :, :, i])     # [C,H,W,D]\n",
    "        if b.shape[1]  == D:   return lambda i: np.squeeze(b[0, i, :, :])     # [C,D,H,W]\n",
    "    elif a.ndim == 4:  # [B,H,W,D] or [B,D,H,W]\n",
    "        b = a[0]\n",
    "        if b.shape[-1] == D:   return lambda i: np.squeeze(b[:, :, i])\n",
    "        if b.shape[0]  == D:   return lambda i: np.squeeze(b[i, :, :])\n",
    "    elif a.ndim == 3:  # [H,W,D] or [D,H,W]\n",
    "        if a.shape[-1] == D:   return lambda i: np.squeeze(a[:, :, i])\n",
    "        if a.shape[0]  == D:   return lambda i: np.squeeze(a[i, :, :])\n",
    "    raise ValueError(f\"Can't determine depth axis for shape {a.shape} with D={D}\")\n",
    "\n",
    "def window_ct_hu(img, wl=-600, ww=1500):\n",
    "    lo, hi = wl - ww/2, wl + ww/2\n",
    "    x = np.nan_to_num(img.astype(np.float32), nan=lo)\n",
    "    x = np.clip(x, lo, hi)\n",
    "    return (x - lo) / (hi - lo + 1e-6)\n",
    "\n",
    "def normalize_for_display(img):\n",
    "    x = np.nan_to_num(img.astype(np.float32), nan=0.0)\n",
    "    vmin, vmax = np.percentile(x, [1, 99])\n",
    "    looks_like_hu = (x.min() < -500) or (x.max() > 200) or (vmax - vmin > 800)\n",
    "    return window_ct_hu(x) if looks_like_hu else (\n",
    "        np.zeros_like(x)+0.5 if vmax - vmin < 1e-6\n",
    "        else (np.clip(x, vmin, vmax) - vmin) / (vmax - vmin + 1e-6)\n",
    "    )\n",
    "\n",
    "# ---- orientation: flip TB then rotate -90Â° (clockwise)\n",
    "def reorient_2d(a):\n",
    "    return np.rot90(np.flipud(a), k=3)\n",
    "\n",
    "# -------- data prep --------\n",
    "pred_np = to_np(output_tensor.argmax(0))  # [H,W,D]\n",
    "D = pred_np.shape[-1]\n",
    "get_img = make_depth_slicer(imgs, D)\n",
    "get_gt  = make_depth_slicer(mask, D) if ('mask' in globals() and mask is not None) else None\n",
    "\n",
    "# -------- figure & artists --------\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_axis_off()\n",
    "\n",
    "# initial (slice 0)\n",
    "ct0 = reorient_2d(normalize_for_display(get_img(0)))\n",
    "im_ct   = ax.imshow(ct0, cmap=\"bone\", vmin=0, vmax=1, zorder=0)\n",
    "\n",
    "p0 = reorient_2d(pred_np[:, :, 0])\n",
    "im_pred = ax.imshow(np.ma.masked_where(p0 <= 0, p0.astype(float)),\n",
    "                    alpha=0.35, cmap=\"autumn\", interpolation=\"nearest\", zorder=3)\n",
    "\n",
    "im_gt = None\n",
    "if get_gt is not None:\n",
    "    gt0 = reorient_2d(get_gt(0))\n",
    "    im_gt = ax.imshow(np.ma.masked_where(gt0 <= 0, gt0.astype(float)),\n",
    "                      alpha=0.35, cmap=\"winter\", interpolation=\"nearest\", zorder=2)\n",
    "\n",
    "def update(i):\n",
    "    # base CT\n",
    "    ct = reorient_2d(normalize_for_display(get_img(i)))\n",
    "    im_ct.set_data(ct)\n",
    "\n",
    "    # prediction\n",
    "    p = reorient_2d(pred_np[:, :, i])\n",
    "    im_pred.set_data(np.ma.masked_where(p <= 0, p.astype(float)))\n",
    "\n",
    "    # GT\n",
    "    if im_gt is not None:\n",
    "        gt = reorient_2d(get_gt(i))\n",
    "        im_gt.set_data(np.ma.masked_where(gt <= 0, gt.astype(float)))\n",
    "\n",
    "    return tuple([x for x in (im_ct, im_pred, im_gt) if x is not None])\n",
    "\n",
    "def init():\n",
    "    return update(0)\n",
    "\n",
    "ani = FuncAnimation(fig,\n",
    "                    update,\n",
    "                    frames=range(1, D, 2),   # start at 1 to avoid duplicate first frame\n",
    "                    init_func=init,\n",
    "                    interval=60,\n",
    "                    blit=False,\n",
    "                    repeat=False)\n",
    "\n",
    "# Show animation only (avoid extra static image)\n",
    "html = ani.to_jshtml()\n",
    "plt.close(fig)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Per-split evaluation (now also returns subject_ids) ----------\n",
    "@torch.no_grad()\n",
    "def evaluate_split_dice(\n",
    "    dataset: tio.SubjectsDataset,\n",
    "    model: torch.nn.Module,\n",
    "    device: Optional[torch.device] = None,\n",
    "    patch_size=(96, 96, 96),\n",
    "    patch_overlap=(32, 32, 32),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    tumor_idx=1,\n",
    "    pos_thresh=0.5,\n",
    "    tumor_positive_values=(1,),\n",
    "    overlap_mode='hann',\n",
    "    postprocess_lcc: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[float, List[float], List[str]]:\n",
    "    \"\"\"\n",
    "    Iterates subjects in `dataset`, predicts full volume, computes per-subject Dice.\n",
    "    Returns (mean_dice, per_subject_dice_list, subject_ids).\n",
    "    \"\"\"\n",
    "    dices: List[float] = []\n",
    "    subject_ids: List[str] = []\n",
    "    model = model.eval()\n",
    "    if device is None:\n",
    "        device = getattr(model, 'device', None) or next(model.parameters()).device\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        subject = dataset[i]\n",
    "        sid = subject.get('subject_id', f'case_{i}')\n",
    "        subject_ids.append(sid)\n",
    "\n",
    "        # predict full logits in native orientation/spacing\n",
    "        logits_full = predict_full_volume_tio(\n",
    "            subject, model, device=device,\n",
    "            patch_size=patch_size, patch_overlap=patch_overlap,\n",
    "            batch_size=batch_size, num_workers=num_workers,\n",
    "            overlap_mode=overlap_mode,\n",
    "        )  # [C, H, W, D]\n",
    "\n",
    "        # binarize prediction & GT\n",
    "        pred_bin = logits_to_binary_pred(logits_full, tumor_idx=tumor_idx, pos_thresh=pos_thresh)  # [H,W,D]\n",
    "        if postprocess_lcc:\n",
    "            pred_bin = largest_component(pred_bin)\n",
    "        gt_bin   = subject_label_to_binary(subject, tumor_positive_values=tumor_positive_values)   # [H,W,D]\n",
    "\n",
    "        # Dice\n",
    "        d = dice_binary(gt_bin, pred_bin)\n",
    "        dices.append(d)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{sid}] Dice: {d:.4f}\")\n",
    "\n",
    "    mean_dice = float(np.mean(dices)) if dices else 0.0\n",
    "    if verbose:\n",
    "        print(f\"\\nMean Dice over {len(dices)} subjects: {mean_dice:.4f}\")\n",
    "    return mean_dice, dices, subject_ids\n",
    "\n",
    "\n",
    "# ---------- Pretty printer for split results ----------\n",
    "def print_split_report(subject_ids: List[str], dices: List[float], title: str = \"Split\"):\n",
    "    assert len(subject_ids) == len(dices)\n",
    "    print(f\"\\n=== {title} Dice Report ===\")\n",
    "    for sid, d in zip(subject_ids, dices):\n",
    "        print(f\"[{sid}] Dice: {d:.4f}\")\n",
    "    dices_np = np.asarray(dices, dtype=float)\n",
    "    print(f\"\\n{title} summary:\")\n",
    "    print(f\"  Mean : {dices_np.mean():.4f}\")\n",
    "    print(f\"  Median: {np.median(dices_np):.4f}\")\n",
    "    print(f\"  Std  : {dices_np.std(ddof=0):.4f}\")\n",
    "    print(f\"  Min  : {dices_np.min():.4f}\")\n",
    "    print(f\"  Max  : {dices_np.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# ---------- case-id helpers ----------\n",
    "def _extract_case_id_from_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Accepts a filename like:\n",
    "      - Lung_003_0000.nii.gz  -> '003'\n",
    "      - Lung_003.nii.gz       -> '003'\n",
    "    Falls back to the stem if pattern isn't found.\n",
    "    \"\"\"\n",
    "    # Try CT-style: Lung_xxx_0000.nii.gz\n",
    "    m = re.match(r\"^Lung_([^_]+)_0000\\.nii(\\.gz)?$\", name, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    # Try label-style: Lung_xxx.nii.gz\n",
    "    m = re.match(r\"^Lung_([^_]+)\\.nii(\\.gz)?$\", name, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    # Fallback: return the stem (without .nii/.nii.gz)\n",
    "    # e.g., name='weirdname.nii.gz' -> 'weirdname'\n",
    "    stem = re.sub(r\"\\.nii(\\.gz)?$\", \"\", name, flags=re.IGNORECASE)\n",
    "    return stem\n",
    "\n",
    "def get_case_id_from_subject(subject: tio.Subject) -> str:\n",
    "    \"\"\"\n",
    "    Prefer CT path; if missing, try Label path. Returns a concise ID (e.g., '003').\n",
    "    \"\"\"\n",
    "    # TorchIO stores path at subject['CT'][tio.PATH] and also as subject['CT'].path\n",
    "    ct_path = None\n",
    "    if 'CT' in subject:\n",
    "        try:\n",
    "            ct_path = Path(subject['CT'][tio.PATH])\n",
    "        except Exception:\n",
    "            ct_path = Path(getattr(subject['CT'], \"path\", \"\")) if hasattr(subject['CT'], \"path\") else None\n",
    "\n",
    "    if ct_path is None or not str(ct_path):\n",
    "        # Fallback to label\n",
    "        lab_path = None\n",
    "        if 'Label' in subject:\n",
    "            try:\n",
    "                lab_path = Path(subject['Label'][tio.PATH])\n",
    "            except Exception:\n",
    "                lab_path = Path(getattr(subject['Label'], \"path\", \"\")) if hasattr(subject['Label'], \"path\") else None\n",
    "        if lab_path is not None and str(lab_path):\n",
    "            return _extract_case_id_from_name(lab_path.name)\n",
    "\n",
    "    if ct_path is not None and str(ct_path):\n",
    "        return _extract_case_id_from_name(ct_path.name)\n",
    "\n",
    "    # Last resort: subject_id field or a generated name\n",
    "    return str(subject.get('subject_id', 'unknown'))\n",
    "\n",
    "# ---------- (reuse your existing predict/eval functions) ----------\n",
    "@torch.no_grad()\n",
    "def predict_full_volume_tio(\n",
    "    subject: tio.Subject,\n",
    "    model: torch.nn.Module,\n",
    "    device: Optional[torch.device] = None,\n",
    "    patch_size=(96, 96, 96),\n",
    "    patch_overlap=(32, 32, 32),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    overlap_mode='hann',\n",
    ") -> torch.Tensor:\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = getattr(model, 'device', None) or next(model.parameters()).device\n",
    "\n",
    "    sampler = tio.inference.GridSampler(subject, patch_size=patch_size, patch_overlap=patch_overlap)\n",
    "    aggregator = tio.inference.GridAggregator(sampler, overlap_mode=overlap_mode)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        sampler, batch_size=batch_size, num_workers=num_workers,\n",
    "        pin_memory=(device.type == 'cuda'), persistent_workers=(num_workers > 0)\n",
    "    )\n",
    "    for batch in loader:\n",
    "        x = batch['CT'][tio.DATA].to(device)        # [B,1,w,h,d]\n",
    "        logits = model(x)                            # [B,C,w,h,d]\n",
    "        aggregator.add_batch(logits.detach().cpu(), batch[tio.LOCATION])\n",
    "\n",
    "    return aggregator.get_output_tensor()            # [C,W,H,D]\n",
    "\n",
    "def logits_to_binary_pred(logits_chwhd: torch.Tensor, tumor_idx: int = 1, pos_thresh: float = 0.5) -> np.ndarray:\n",
    "    C = logits_chwhd.shape[0]\n",
    "    if C == 1:\n",
    "        prob = torch.sigmoid(logits_chwhd[0])\n",
    "        pred = (prob > pos_thresh)\n",
    "    else:\n",
    "        pred = (torch.argmax(logits_chwhd, dim=0) == tumor_idx)\n",
    "    return pred.numpy().astype(np.uint8)\n",
    "\n",
    "def subject_label_to_binary(subject: tio.Subject, tumor_positive_values=(1,)) -> np.ndarray:\n",
    "    lab = subject['Label'][tio.DATA]         # [1,W,H,D]\n",
    "    lab = lab[0].detach().cpu().numpy()      # [W,H,D]\n",
    "    return np.isin(lab, tumor_positive_values).astype(np.uint8)\n",
    "\n",
    "def dice_binary(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6, empty_ok_as: float = 1.0) -> float:\n",
    "    y_true = (y_true > 0).astype(np.uint8)\n",
    "    y_pred = (y_pred > 0).astype(np.uint8)\n",
    "    t_sum, p_sum = y_true.sum(), y_pred.sum()\n",
    "    if t_sum == 0 and p_sum == 0:\n",
    "        return float(empty_ok_as)\n",
    "    inter = (y_true & y_pred).sum()\n",
    "    return float((2.0 * inter + eps) / (t_sum + p_sum + eps))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_split_dice(\n",
    "    dataset: tio.SubjectsDataset,\n",
    "    model: torch.nn.Module,\n",
    "    device: Optional[torch.device] = None,\n",
    "    patch_size=(96,96,96),\n",
    "    patch_overlap=(32,32,32),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    tumor_idx=1,\n",
    "    pos_thresh=0.5,\n",
    "    tumor_positive_values=(1,),\n",
    "    overlap_mode='hann',\n",
    "    print_per_case: bool = True,\n",
    ") -> Tuple[float, List[float], List[str]]:\n",
    "    if device is None:\n",
    "        device = getattr(model, 'device', None) or next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    dices: List[float] = []\n",
    "    ids: List[str] = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        subject = dataset[i]\n",
    "        case_id = get_case_id_from_subject(subject)  # <-- actual case number from filename\n",
    "        ids.append(case_id)\n",
    "\n",
    "        logits_full = predict_full_volume_tio(\n",
    "            subject, model, device=device,\n",
    "            patch_size=patch_size, patch_overlap=patch_overlap,\n",
    "            batch_size=batch_size, num_workers=num_workers,\n",
    "            overlap_mode=overlap_mode\n",
    "        )  # [C,W,H,D]\n",
    "\n",
    "        pred_bin = logits_to_binary_pred(logits_full, tumor_idx=tumor_idx, pos_thresh=pos_thresh)\n",
    "        gt_bin   = subject_label_to_binary(subject, tumor_positive_values=tumor_positive_values)\n",
    "\n",
    "        d = dice_binary(gt_bin, pred_bin)\n",
    "        dices.append(d)\n",
    "\n",
    "        if print_per_case:\n",
    "            print(f\"[case {case_id}] Dice: {d:.4f}\")\n",
    "\n",
    "    mean_dice = float(np.mean(dices)) if dices else 0.0\n",
    "    if print_per_case:\n",
    "        print(f\"\\nMean Dice over {len(dices)} subjects: {mean_dice:.4f}\")\n",
    "    return mean_dice, dices, ids\n",
    "\n",
    "# =======================\n",
    "# Usage\n",
    "# =======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "common = dict(\n",
    "    patch_size=(96,96,96),\n",
    "    patch_overlap=(32,32,32),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    overlap_mode='hann',\n",
    "    tumor_idx=1,\n",
    "    pos_thresh=0.5,              # fixed sigmoid threshold\n",
    "    tumor_positive_values=(1,),\n",
    "    print_per_case=True,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation split:\")\n",
    "mean_val, val_dices, val_ids = evaluate_split_dice(val_dataset, model, device=device, **common)\n",
    "\n",
    "print(\"\\nTest split:\")\n",
    "mean_test, test_dices, test_ids = evaluate_split_dice(test_dataset, model, device=device, **common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
